{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "np.random.seed(23)\n",
    "torch.manual_seed(23)\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "NUM_PTS = 971\n",
    "CROP_SIZE = 128\n",
    "SUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-christopher",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMinSideToSize(object):\n",
    "    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n",
    "        self.size = np.asarray(size, dtype=np.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w, _ = sample[self.elem_name].shape\n",
    "        if h > w:\n",
    "            f = self.size[0] / w\n",
    "        else:\n",
    "            f = self.size[1] / h\n",
    "\n",
    "        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n",
    "        sample[\"scale_coef\"] = f\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2).float()\n",
    "            landmarks = landmarks * f\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformByKeys(object):\n",
    "    def __init__(self, transform, names):\n",
    "        self.transform = transform\n",
    "        self.names = set(names)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for name in self.names:\n",
    "            if name in sample:\n",
    "                sample[name] = self.transform(sample[name])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropCenter(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample[self.elem_name]\n",
    "        h, w, _ = img.shape\n",
    "        margin_h = (h - self.size) // 2\n",
    "        margin_w = (w - self.size) // 2\n",
    "        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "        sample[\"crop_margin_x\"] = margin_w\n",
    "        sample[\"crop_margin_y\"] = margin_h\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThousandLandmarksDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms, split=\"train\"):\n",
    "        super(ThousandLandmarksDataset, self).__init__()\n",
    "        self.root = root\n",
    "        landmark_file_name = os.path.join(root, 'landmarks.csv') if split != \"test\" \\\n",
    "            else os.path.join(root, \"test_points.csv\")\n",
    "        images_root = os.path.join(root, \"images\")\n",
    "\n",
    "        self.image_names = []\n",
    "        self.landmarks = []\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            num_lines = sum(1 for line in fp)\n",
    "        num_lines -= 1  # header\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            for i, line in tqdm.tqdm(enumerate(fp), total=num_lines + 1):\n",
    "                if i == 0:\n",
    "                    continue  # skip header\n",
    "                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n",
    "                    break  # reached end of train part of data\n",
    "                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n",
    "                    continue  # has not reached start of val part of data\n",
    "                elements = line.strip().split(\"\\t\")\n",
    "                image_name = os.path.join(images_root, elements[0])\n",
    "                self.image_names.append(image_name)\n",
    "\n",
    "                if split in (\"train\", \"val\"):\n",
    "                    landmarks = list(map(np.int, elements[1:]))\n",
    "                    landmarks = np.array(landmarks, dtype=np.int).reshape((len(landmarks) // 2, 2))\n",
    "                    self.landmarks.append(landmarks)\n",
    "\n",
    "        if split in (\"train\", \"val\"):\n",
    "            self.landmarks = torch.as_tensor(self.landmarks)\n",
    "        else:\n",
    "            self.landmarks = None\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        if self.landmarks is not None:\n",
    "            landmarks = self.landmarks[idx]\n",
    "            sample[\"landmarks\"] = landmarks\n",
    "\n",
    "        image = cv2.imread(self.image_names[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        sample[\"image\"] = image\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_landmarks(landmarks, f, margins):\n",
    "    dx, dy = margins\n",
    "    landmarks[:, 0] += dx\n",
    "    landmarks[:, 1] += dy\n",
    "    landmarks /= f\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_landmarks_batch(landmarks, fs, margins_x, margins_y):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-regard",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"Training...\"):\n",
    "        images = batch[\"image\"].to(device) # B * 3 * CROP_SIZE * CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"] # B * (2 * NUM_PTS)\n",
    "        \n",
    "        pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks, reduction=\"mean\")\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"Validation...\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks, reduction=\"mean\")\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    for i, batch in enumerate(tqdm.tqdm(loader, total=len(loader), desc=\"Test prediction...\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2)) # B * NUM_PTS * 2\n",
    "        \n",
    "        fs = batch[\"scale_coef\"].numpy() # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy() # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy() # B\n",
    "        prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y) # B * NUM_PTS * 2\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-designation",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define transforms and load data\n",
    "train_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]), (\"image\",))\n",
    "])\n",
    "\n",
    "print(\"Data loading...\")\n",
    "train_dataset = ThousandLandmarksDataset(\"../input/made-cv-2021-contest-01-facial-landmarks/contest01_data/train\", train_transforms, split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = ThousandLandmarksDataset(\"../input/made-cv-2021-contest-01-facial-landmarks/contest01_data/train\", train_transforms, split=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=False, drop_last=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkModel(nn.Module):\n",
    "    def __init__(self, pts=2 * NUM_PTS):\n",
    "        super(LandmarkModel, self).__init__()\n",
    "        self.model = models.resnet34(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, pts)\n",
    "        self.fc_1 = nn.Linear(pts, 25 * pts)\n",
    "        self.fc_2 = nn.Linear(25 * pts, pts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model creation\")\n",
    "model = LandmarkModel()\n",
    "\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.fc.requires_grad_(True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "loss_fn = fnn.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train and validation\n",
    "print(\"Train network\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, loss_fn, device=device)\n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.2}\\tval loss: {:5.2}\".format(epoch + 1, train_loss, val_loss))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(os.path.join(\"./\", \"landmarks_best.pth\"), \"wb\") as fp:\n",
    "                torch.save(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict\n",
    "test_dataset = ThousandLandmarksDataset(\"../input/made-cv-2021-contest-01-facial-landmarks/contest01_data/test\", train_transforms, split=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True,\n",
    "                                 shuffle=False, drop_last=False)\n",
    "\n",
    "with open(\"./landmarks_best.pth\", \"rb\") as fp:\n",
    "        best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "        model.load_state_dict(best_state_dict)\n",
    "        \n",
    "test_predictions = predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                     \"landmarks\": test_predictions}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(path_to_data, test_predictions, path_to_submission_file):\n",
    "    test_dir = os.path.join(path_to_data, \"test\")\n",
    "\n",
    "    output_file = path_to_submission_file\n",
    "    wf = open(output_file, 'w')\n",
    "    wf.write(SUBMISSION_HEADER)\n",
    "\n",
    "    mapping_path = os.path.join(test_dir, 'test_points.csv')\n",
    "    mapping = pd.read_csv(mapping_path, delimiter='\\t')\n",
    "\n",
    "    for i, row in mapping.iterrows():\n",
    "        file_name = row[0]\n",
    "        point_index_list = np.array(eval(row[1]))\n",
    "        points_for_image = test_predictions[i]\n",
    "        needed_points = points_for_image[point_index_list].astype(np.int)\n",
    "        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\"../input/made-cv-2021-contest-01-facial-landmarks/contest01_data\", test_predictions, \"submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
